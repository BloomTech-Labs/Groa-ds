{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from time import time\n",
    "from random import randint\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import pandas as pd\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 RuxitSynthetic/1.0 v316542848 t18859'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/browse/games/release-date/available/vita/date?page=8\n"
     ]
    }
   ],
   "source": [
    "# Get all game urls\n",
    "categories = ['ps4', 'xboxone', 'switch', 'pc', 'ios', 'wii-u', '3ds', 'vita']\n",
    "\n",
    "# List to store game urls\n",
    "game_urls = []\n",
    "\n",
    "# For each category\n",
    "for category in categories:\n",
    "    url = 'https://www.metacritic.com/browse/games/release-date/available/'+category+'/date'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # Pause the loop\n",
    "    sleep(randint(8,15))\n",
    "    # Monitor the loop\n",
    "    print(url)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Get the game containers\n",
    "    game_containers = page_html.find_all('li', class_='product game_product')\n",
    "    # First and last games have different class names\n",
    "    game_containers.append(page_html.find('li', class_='product game_product first_product'))\n",
    "    game_containers.append(page_html.find('li', class_='product game_product last_product'))\n",
    "    \n",
    "    for container in game_containers:\n",
    "        url = container.a['href']\n",
    "        # Append to list if not exists\n",
    "        if url not in urls:\n",
    "            game_urls.append(url)\n",
    "    \n",
    "    # Go to the next page\n",
    "    # If there is a next button\n",
    "    if page_html.find('span', class_='flipper next'):\n",
    "        # Get the reference of the next page button\n",
    "        next_page = page_html.find('span', class_='flipper next').a\n",
    "        # Loop through the next pages while the button has a reference\n",
    "        while next_page:\n",
    "            # Next page url\n",
    "            next_page_url = next_page['href']\n",
    "            # Download the page\n",
    "            response = requests.get('https://www.metacritic.com'+next_page_url, headers=headers)\n",
    "            \n",
    "            # Pause the loop\n",
    "            sleep(randint(8,15))\n",
    "            # Monitor the loop\n",
    "            print(next_page_url)\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Get the game containers\n",
    "            game_containers = page_html.find_all('li', class_='product game_product')\n",
    "            # First and last games have different class names\n",
    "            game_containers.append(page_html.find('li', class_='product game_product first_product'))\n",
    "            game_containers.append(page_html.find('li', class_='product game_product last_product'))\n",
    "\n",
    "            # Get the game url\n",
    "            for container in game_containers:\n",
    "                url = container.a['href']\n",
    "                # Append to list if not exists\n",
    "                if url not in urls:\n",
    "                    game_urls.append(url)\n",
    "            \n",
    "            # Get the next page\n",
    "            # The while loop stops if next_page==None\n",
    "            next_page = page_html.find('span', class_='flipper next').a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_urls_df = pd.DataFrame({\n",
    "    'url':game_urls\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77817, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_urls_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_urls_df.to_csv('game_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the scraped data\n",
    "urls = []\n",
    "names = []\n",
    "release_dates = []\n",
    "game_platforms = []\n",
    "descriptions = []\n",
    "developers = []\n",
    "genres_list = []\n",
    "online_players_list = []\n",
    "\n",
    "for game in game_urls:\n",
    "    # Download the page\n",
    "    response = requests.get('https://www.metacritic.com'+game+'/details', headers=headers)\n",
    "    # Pause the loop\n",
    "    sleep(randint(8,15))\n",
    "    # Monitor the loop\n",
    "    print(game)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Save the game url\n",
    "    # It will be used to assign an id for ratings & reviews\n",
    "    urls.append(game)\n",
    "\n",
    "    # The name\n",
    "    name = page_html.h1.text\n",
    "    names.append(name)\n",
    "\n",
    "    # The release date\n",
    "    date_container = page_html.find('li', class_='summary_detail release_data')\n",
    "    release_date = date_container.find('span', class_='data').text\n",
    "    release_dates.append(release_date)\n",
    "\n",
    "    # The game platforms\n",
    "    primary_platform = page_html.find('span', class_='platform').text.replace('\\n','').strip()\n",
    "    platform_container = page_html.find('li', class_='summary_detail product_platforms')\n",
    "    other_platforms = platform_container.find('span', class_='data').text.replace('\\n', '').replace(' ','')\n",
    "    platforms = primary_platform+','+other_platforms\n",
    "    game_platforms.append(platforms)\n",
    "\n",
    "    # The description\n",
    "    summary_container = page_html.find('div', class_='summary_detail product_summary')\n",
    "    description = summary_container.find('span', class_='data').text\n",
    "    descriptions.append(description)\n",
    "\n",
    "    # Developer, genres, and number of online players\n",
    "    details_container = page_html.find_all('div', class_='product_details')[1]\n",
    "    details_values = details_container.find_all('td')\n",
    "    developer = details_values[1].text\n",
    "    developers.append(developer)\n",
    "    genres = details_values[2].text.replace('\\r\\n', '').replace(' ', '')\n",
    "    genres_list.append(genres)\n",
    "    online_players = details_values[3].text\n",
    "    online_players_list.append(online_players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store into a dataframe\n",
    "games = pd.DataFrame({\n",
    "    'url' : urls,\n",
    "    'name' : names,\n",
    "    'release_date' : release_dates,\n",
    "    'platforms' : game_platforms,\n",
    "    'description' : descriptions,\n",
    "    'developer' : developers,\n",
    "    'genres' : genres_list,\n",
    "    'number_online_players' : online_players_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a csv for further processing if needed\n",
    "games.to_csv('games.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game ratings & reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the scraped data\n",
    "urls = []\n",
    "user_names = []\n",
    "ratings = []\n",
    "reviews = []\n",
    "review_dates = []\n",
    "\n",
    "for game in game_urls:\n",
    "    # Download the page\n",
    "    response = requests.get('https://www.metacritic.com'+game+'/user-reviews', headers=headers)\n",
    "    # Pause the loop\n",
    "    sleep(randint(8,15))\n",
    "    # Monitor the loop\n",
    "    print(game)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Reviews section\n",
    "    all_reviews = page_html.find('ol', class_='reviews user_reviews')\n",
    "\n",
    "    # If there is a user reviews section\n",
    "    if all_reviews:\n",
    "    # The users\n",
    "    all_users = all_reviews.find_all('div', class_='name')\n",
    "    for i in range(len(all_users)):\n",
    "        user_names.append(all_users[i].text.replace('\\n', ''))\n",
    "        # Save the game url\n",
    "        urls.append(game)\n",
    "\n",
    "    # The ratings\n",
    "    all_ratings = all_reviews.find_all('div', class_='review_grade')\n",
    "    for i in range(len(all_ratings)):\n",
    "        rating = all_ratings[i].text.replace('\\n', '')\n",
    "        rating = int(rating)/2\n",
    "        ratings.append(rating)\n",
    "\n",
    "    # The reviews\n",
    "    all_user_reviews = all_reviews.find_all('div', class_='review_body')\n",
    "    for i in range(len(all_user_reviews)):\n",
    "        if all_user_reviews[i].span:\n",
    "            review = all_user_reviews[i].span.text\n",
    "            reviews.append(review)\n",
    "        else:\n",
    "            reviews.append(' ')\n",
    "\n",
    "    # The review dates\n",
    "    all_dates = all_reviews.find_all('div', class_='date')\n",
    "    for i in range(len(all_dates)):\n",
    "        review_date = all_dates[i].text\n",
    "        review_dates.append(review_date)\n",
    "\n",
    "    # Go to the next page\n",
    "    # If there is a next button\n",
    "    if page_html.find('span', class_='flipper next'):\n",
    "        # Get the reference of the next page button\n",
    "        next_page = page_html.find('span', class_='flipper next').a\n",
    "        # Loop through the next pages while the button has a reference\n",
    "        while next_page:\n",
    "            # Next page url\n",
    "            next_page_url = next_page['href']\n",
    "            # Download the page\n",
    "            response = requests.get('https://www.metacritic.com'+next_page_url, headers=headers)\n",
    "\n",
    "            # Pause the loop\n",
    "            sleep(randint(8,15))\n",
    "            # Monitor the loop\n",
    "            print(next_page_url)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Reviews section\n",
    "            all_reviews = page_html.find('ol', class_='reviews user_reviews')\n",
    "\n",
    "            # If there is a user reviews section\n",
    "            if all_reviews:\n",
    "                # The users\n",
    "                all_users = all_reviews.find_all('div', class_='name')\n",
    "                for i in range(len(all_users)):\n",
    "                    user_names.append(all_users[i].text.replace('\\n', ''))\n",
    "                    # Save the game url\n",
    "                    urls.append(game)\n",
    "\n",
    "                # The ratings\n",
    "                all_ratings = all_reviews.find_all('div', class_='review_grade')\n",
    "                for i in range(len(all_ratings)):\n",
    "                    rating = all_ratings[i].text.replace('\\n', '')\n",
    "                    rating = int(rating)/2\n",
    "                    ratings.append(rating)\n",
    "\n",
    "                # The reviews\n",
    "                all_user_reviews = all_reviews.find_all('div', class_='review_body')\n",
    "                for i in range(len(all_user_reviews)):\n",
    "                    if all_user_reviews[i].span:\n",
    "                        review = all_user_reviews[i].span.text\n",
    "                        reviews.append(review)\n",
    "                    else:\n",
    "                        reviews.append(' ')\n",
    "\n",
    "                # The review dates\n",
    "                all_dates = all_reviews.find_all('div', class_='date')\n",
    "                for i in range(len(all_dates)):\n",
    "                    review_date = all_dates[i].text\n",
    "                    review_dates.append(review_date)\n",
    "\n",
    "            # Get the next page\n",
    "            # The while loop stops if next_page==None\n",
    "            next_page = page_html.find('span', class_='flipper next').a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ratings = pd.DataFrame({\n",
    "    'url' : urls,\n",
    "    'user_name' : user_names,\n",
    "    'rating' : ratings,\n",
    "    'review' : reviews,\n",
    "    'review_date' : review_dates\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ratings.to_csv('game_ratings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
