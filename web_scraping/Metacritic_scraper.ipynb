{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TV urls (series overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5L-PH0LJDv2"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from time import time\n",
    "from random import randint\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import pandas as pd\n",
    "\n",
    "# Use your own User-Agent\n",
    "headers = {'User-Agent':'...'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqQfn23-vSiu"
   },
   "outputs": [],
   "source": [
    "# Get the tv urls for the overview details page\n",
    "# Lists to store the scraped data in\n",
    "tv_urls = []\n",
    "\n",
    "# Get series urls - the overview of the series \n",
    "# Preparing the monitoring of the loop\n",
    "start_time = time()\n",
    "req = 0\n",
    "\n",
    "# The loop\n",
    "categories = ['new-series', 'returning-series', 'special-event']\n",
    "# For every category\n",
    "for category in categories:\n",
    "    if category=='returning-series':\n",
    "    pages = range(7)\n",
    "    else:\n",
    "    pages = [0]\n",
    "\n",
    "    # For every page\n",
    "    for page in pages:\n",
    "    if page==0:\n",
    "        response = requests.get('https://www.metacritic.com/browse/tv/release-date/'+category+'/date', headers=headers)\n",
    "    else:\n",
    "        response = requests.get('https://www.metacritic.com/browse/tv/release-date/'+category+'/date?page='+str(page), headers=headers)\n",
    "\n",
    "    # Pause the loop\n",
    "    sleep(randint(8,15))\n",
    "\n",
    "    # Monitor the requests\n",
    "    req += 1\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request:{}; Frequency: {} requests/s'.format(req, req/elapsed_time))\n",
    "    clear_output(wait = True)\n",
    "\n",
    "    # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(req, response.status_code))\n",
    "\n",
    "    # Break the loop if the number of requests is greater than expected\n",
    "    if req > 10:\n",
    "        warn('Number of requests was greater than expected.')\n",
    "\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Select all the tvshow containters from a single page\n",
    "    first_show = page_html.find('li', class_='product season_product first_product')\n",
    "    show_containers = page_html.find_all('li', class_='product season_product')\n",
    "    last_show = page_html.find('li', class_='product season_product last_product')\n",
    "\n",
    "    # Extract the page of tvshow from individual tvshow container\n",
    "    # First container\n",
    "    # TV Show page url\n",
    "    show_url = first_show.a['href'].split('/season-', 1)[0]\n",
    "    if show_url not in tv_urls:\n",
    "        tv_urls.append(show_url)\n",
    "\n",
    "    # Middle containers\n",
    "    for container in show_containers:\n",
    "        show_url = container.a['href'].split('/season-', 1)[0]\n",
    "        if show_url not in tv_urls:\n",
    "        tv_urls.append(show_url)\n",
    "\n",
    "    # Last container\n",
    "    # TV Show page url\n",
    "    show_url = last_show.a['href'].split('/season-', 1)[0]\n",
    "    if show_url not in tv_urls:\n",
    "        tv_urls.append(show_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDIyqYVzVM56"
   },
   "outputs": [],
   "source": [
    "# Save tv_urls into csv, so that we don't need to make the future requests\n",
    "tv_urls_df = pd.DataFrame({'tv_url': tv_urls})\n",
    "tv_urls_df.to_csv('tv_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TV series overview ratings & reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "Ek0nQyMSClKP",
    "outputId": "2e40d18d-8af9-4ebb-b0ee-c93596d4d2fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tv/love-fraud', '/tv/in-my-skin']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the `tv_urls.csv` file\n",
    "tv_urls = pd.read_csv('tv_urls.csv')\n",
    "\n",
    "# Save tv_url in a list\n",
    "tv_urls_list = tv_urls['tv_url'].tolist()\n",
    "tv_urls_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TBNQMMy4Dt4_"
   },
   "outputs": [],
   "source": [
    "# Loop through all tv_urls and get title, date, user names, user ratings, and user reviews\n",
    "# This takes the information for the overview of the tv series\n",
    "\n",
    "# Lists to store the scraped data\n",
    "titles = []\n",
    "dates = []\n",
    "user_names = []\n",
    "ratings = []\n",
    "reviews = []\n",
    "review_dates = []\n",
    "\n",
    "\n",
    "# Functions to make the code more efficient\n",
    "def monitor_loop():\n",
    "    \"\"\"\n",
    "    Function to pause the loop and monitor the requests\n",
    "    \"\"\"\n",
    "    # Pause the loop\n",
    "    sleep(randint(10,30))\n",
    "    \n",
    "    # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        message = warn('Status code: {}'.format(response.status_code))\n",
    "        return message\n",
    "\n",
    "def get_title_date():\n",
    "    \"\"\"\n",
    "    Gets the title and the release date\n",
    "    \"\"\"\n",
    "    # The title\n",
    "    title = page_html.h1.text\n",
    "    # The release date\n",
    "    release_date = page_html.find('span', class_='release_date').find('span', class_=None).text\n",
    "    return title, release_date\n",
    "\n",
    "def get_user_rating_review():\n",
    "    \"\"\"\n",
    "    Gets the user name, rating, review, and review_date\n",
    "    Adds the scraped data to the lists\n",
    "    \"\"\"\n",
    "    # Add the title to the list\n",
    "    titles.append(title)\n",
    "    # Add the release date to the list\n",
    "    dates.append(release_date)\n",
    "    # Get the user name and add it to the list\n",
    "    user_name = container.a.text\n",
    "    user_names.append(user_name)\n",
    "    # Get the rating and add it to the list\n",
    "    rating = container.find('div', class_='left fl').text.replace('\\n','')\n",
    "    rating = int(rating)/2\n",
    "    ratings.append(rating)\n",
    "    # Get the review and add it to the list\n",
    "    review = container.find('div', class_='review_body').text.replace('\\n', '')\n",
    "    reviews.append(review)\n",
    "    # Get the review date and add it to the list\n",
    "    review_date = container.find('span', class_='date').text\n",
    "    review_dates.append(review_date)\n",
    "    return titles, dates, user_names, ratings, reviews, review_dates\n",
    "    \n",
    "# The loop\n",
    "# For each url of the series\n",
    "for url in tv_urls_list:\n",
    "    # Monitor the url scraped\n",
    "    print(url)\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # Get the html page\n",
    "    response = requests.get('https://www.metacritic.com'+url+'/user-reviews', headers=headers)\n",
    "    # Monitor the loop\n",
    "    monitor_loop()\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "    # Get the reviews containers\n",
    "    review_containers = page_html.find_all('div', class_='review pad_top1')\n",
    "    \n",
    "    # If any review_container, get the data\n",
    "    if review_containers:\n",
    "        # The title and release date\n",
    "        title, release_date = get_title_date()\n",
    "        # Get the data for each review\n",
    "        for container in review_containers:\n",
    "            titles, dates, user_names, ratings, reviews, review_dates = get_user_rating_review()\n",
    "\n",
    "        # Go to the next page (if any)\n",
    "        # Runs only if there is a `next` button\n",
    "        if page_html.find('span', class_='flipper next'):\n",
    "            # Get the page url\n",
    "            next_page = page_html.find('span', class_='flipper next').a\n",
    "            # The while loop runs only if next_page!=None\n",
    "            while next_page:\n",
    "                next_page_url = next_page['href']\n",
    "                # Monitor the page scraped\n",
    "                print(next_page_url)\n",
    "                clear_output(wait = True)\n",
    "                # Download the next page\n",
    "                response = requests.get('https://www.metacritic.com'+next_page_url, headers=headers)\n",
    "                # Monitor the loop\n",
    "                monitor_loop()\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "                # The reviews containers\n",
    "                review_containers = page_html.find_all('div', class_='review pad_top1')\n",
    "                # The data for each review\n",
    "                for container in review_containers:\n",
    "                    get_user_rating_review()\n",
    "                # Get the page url\n",
    "                # The while loop stops if next_page==None\n",
    "                next_page = page_html.find('span', class_='flipper next').a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save series ratings into csv, so that we don't need to make future requests\n",
    "series_ratings = pd.DataFrame({\n",
    "    'title' : titles,\n",
    "    'release_date' : dates,\n",
    "    'user_name' : user_names,\n",
    "    'rating' : ratings,\n",
    "    'review' : reviews,\n",
    "    'review_date' : review_dates\n",
    "})\n",
    "\n",
    "series_ratings.to_csv('series_ratings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Season URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tv/love-fraud', '/tv/in-my-skin']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the `tv_urls.csv` file\n",
    "tv_urls = pd.read_csv('tv_urls.csv')\n",
    "\n",
    "# Save tv_url in a list\n",
    "tv_urls_list = tv_urls['tv_url'].tolist()\n",
    "tv_urls_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_loop():\n",
    "    \"\"\"\n",
    "    Function to pause the loop and monitor the requests\n",
    "    \"\"\"\n",
    "    # Pause the loop\n",
    "    sleep(randint(10,30))\n",
    "    \n",
    "    # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        message = warn('Status code: {}'.format(response.status_code))\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the season urls\n",
    "season_urls = []\n",
    "\n",
    "for url in tv_urls_list:\n",
    "    response = requests.get('https://www.metacritic.com'+url, headers=headers)\n",
    "    # Monitor the loop\n",
    "    monitor_loop()\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Get the url containers\n",
    "    season_urls_containers = page_html.find_all('li', class_='ep_guide_season')\n",
    "    # Get the season url\n",
    "    for container in season_urls_containers:\n",
    "        season_url = container.a['href']\n",
    "        season_urls.append(season_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the seasons urls into csv, so that we don't need to make future requests\n",
    "season_urls_df = pd.DataFrame({'season_url': season_urls})\n",
    "season_urls_df.to_csv('season_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4139"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(season_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Season ratings & reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tv/love-fraud/season-1', '/tv/in-my-skin/season-1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the `season_urls.csv` file\n",
    "season_urls = pd.read_csv('season_urls.csv')\n",
    "\n",
    "# Save season_urls in a list\n",
    "season_urls_list = season_urls['season_url'].tolist()\n",
    "season_urls_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to make the code more efficient\n",
    "def monitor_loop():\n",
    "    \"\"\"\n",
    "    Function to pause the loop and monitor the requests\n",
    "    \"\"\"\n",
    "    # Pause the loop\n",
    "    sleep(randint(10,15))\n",
    "\n",
    "    # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        message = warn('Status code: {}'.format(response.status_code))\n",
    "        return message\n",
    "\n",
    "def get_title_date():\n",
    "    \"\"\"\n",
    "    Gets the title and the release date\n",
    "    \"\"\"\n",
    "    # The title\n",
    "    title = page_html.h1.text + ': ' + page_html.h2.text\n",
    "    # The release date\n",
    "    release_date = page_html.find('ul', class_='summary_details').find_all('span', class_='data')[1].text\n",
    "    return title, release_date\n",
    "\n",
    "def get_user_rating_review():\n",
    "    \"\"\"\n",
    "    Gets the user name, rating, review, and review_date\n",
    "    Adds the scraped data to the lists\n",
    "    \"\"\"\n",
    "    # Get the user names\n",
    "    all_users = all_reviews.find_all('div', class_='name')\n",
    "    for i in range(len(all_users)):        \n",
    "        # Add the title to the list\n",
    "        titles.append(title)\n",
    "        # Add the release date to the list\n",
    "        dates.append(release_date)\n",
    "        user_name = all_users[i].text.replace('\\n', '')\n",
    "        user_names.append(user_name)\n",
    "    # Get the ratings\n",
    "    all_ratings = all_reviews.find_all('div', class_='review_grade')\n",
    "    for i in range(len(all_ratings)):\n",
    "        rating = all_ratings[i].text.replace('\\n', '')\n",
    "        rating = int(rating)/2\n",
    "        ratings.append(rating)\n",
    "    # Get the reviews\n",
    "    all_user_reviews = all_reviews.find_all('div', class_='review_body')\n",
    "    for i in range(len(all_user_reviews)):\n",
    "        if all_user_reviews[i].span:\n",
    "            review = all_user_reviews[i].span.text\n",
    "            reviews.append(review)\n",
    "        else:\n",
    "            reviews.append(' ')\n",
    "    # Get the review dates\n",
    "    all_dates = all_reviews.find_all('div', class_='date')\n",
    "    for i in range(len(all_dates)):\n",
    "        review_date = all_dates[i].text\n",
    "        review_dates.append(review_date)\n",
    "    return titles, dates, user_names, ratings, reviews, review_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tv/strike-back/season-1\n"
     ]
    }
   ],
   "source": [
    "# Loop through all season_urls and get title, date, user names, user ratings, and user reviews\n",
    "# This takes the information for the overview of the seasons\n",
    "\n",
    "# Lists to store the scraped data\n",
    "titles = []\n",
    "dates = []\n",
    "user_names = []\n",
    "ratings = []\n",
    "reviews = []\n",
    "review_dates = []\n",
    "\n",
    "# The loop\n",
    "# For each url of the series\n",
    "for url in season_urls_list:\n",
    "    # Monitor the url scraped\n",
    "    print(url)\n",
    "    clear_output(wait = True)\n",
    "    # Get the html page\n",
    "    response = requests.get('https://www.metacritic.com'+url+'/user-reviews', headers=headers)\n",
    "    # Monitor the loop\n",
    "    monitor_loop()\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Get all user reviews\n",
    "    all_reviews = page_html.find('ol', class_='reviews user_reviews')\n",
    "    # If any all_reviews, get the data\n",
    "    if all_reviews:\n",
    "        # The title and release date\n",
    "        title, release_date = get_title_date()\n",
    "        # Get the data for each review\n",
    "        titles, dates, user_names, ratings, reviews, review_dates = get_user_rating_review()\n",
    "        \n",
    "        # Go to the next page (if any)\n",
    "        # Runs only if there is a `next` button\n",
    "        if page_html.find('span', class_='flipper next'):\n",
    "            # Get the page url\n",
    "            next_page = page_html.find('span', class_='flipper next').a\n",
    "            # The while loop runs only if next_page!=None\n",
    "            while next_page:\n",
    "                next_page_url = next_page['href']\n",
    "                # Monitor the page scraped\n",
    "                print(next_page_url)\n",
    "                clear_output(wait = True)\n",
    "                # Download the next page\n",
    "                response = requests.get('https://www.metacritic.com'+next_page_url, headers=headers)\n",
    "                # Monitor the loop\n",
    "                monitor_loop()\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Get all user reviews\n",
    "                all_reviews = page_html.find('ol', class_='reviews user_reviews')\n",
    "                # If any all_reviews, get the data\n",
    "                if all_reviews:\n",
    "                    # The title and release date\n",
    "                    title, release_date = get_title_date()\n",
    "                    # Get the data for each review\n",
    "                    titles, dates, user_names, ratings, reviews, review_dates = get_user_rating_review()\n",
    "                # Get the reference for the next page\n",
    "                # The while loop stops if next_page==None\n",
    "                next_page = page_html.find('span', class_='flipper next').a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save season ratings into csv, so that we don't need to make future requests\n",
    "season_ratings_2 = pd.DataFrame({\n",
    "    'title' : titles,\n",
    "    'release_date' : dates,\n",
    "    'user_name' : user_names,\n",
    "    'rating' : ratings,\n",
    "    'review' : reviews,\n",
    "    'review_date' : review_dates\n",
    "})\n",
    "\n",
    "season_ratings_2.to_csv('season_ratings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tv/love-fraud/season-1', '/tv/in-my-skin/season-1']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the `season_urls.csv` file\n",
    "season_urls = pd.read_csv('season_urls.csv')\n",
    "\n",
    "# Save season_urls in a list\n",
    "season_urls_list = season_urls['season_url'].tolist()\n",
    "season_urls_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_loop():\n",
    "    \"\"\"\n",
    "    Function to pause the loop and monitor the requests\n",
    "    \"\"\"\n",
    "    # Pause the loop\n",
    "    sleep(randint(10,15))\n",
    "    \n",
    "    # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        message = warn('Status code: {}'.format(response.status_code))\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tv/strike-back/season-1\n"
     ]
    }
   ],
   "source": [
    "# Get the episode urls\n",
    "episode_urls = []\n",
    "\n",
    "for url in season_urls_list:\n",
    "    response = requests.get('https://www.metacritic.com'+url, headers=headers)\n",
    "    # Monitor the loop\n",
    "    monitor_loop()\n",
    "    print(url)\n",
    "    clear_output(wait = True)\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Get the url containers\n",
    "    episode_urls_containers = page_html.find_all('li', class_='ep_guide_item')\n",
    "    # Get the episode url\n",
    "    for container in episode_urls_containers:\n",
    "        if container.text != 'No episode information is currently available for this season':\n",
    "            episode_url = container.a['href']\n",
    "            if episode_url not in episode_urls:\n",
    "                episode_urls.append(episode_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the episode urls into csv, so that we don't need to make future requests\n",
    "episode_urls_df = pd.DataFrame({'episode_url': episode_urls})\n",
    "episode_urls_df.to_csv('episode_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode ratings and reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To run on EC2 instance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tv/love-fraud/season-1/episode-4-episode-4-1048046',\n",
       " '/tv/love-fraud/season-1/episode-3-episode-3-1048045']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the `episode_urls.csv` file\n",
    "episode_urls = pd.read_csv('episode_urls.csv')\n",
    "\n",
    "# Save episode_urls in a list\n",
    "episode_urls_list = episode_urls['episode_url'].tolist()\n",
    "episode_urls_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to make the code more efficient\n",
    "def monitor_loop():\n",
    "    \"\"\"\n",
    "    Function to pause the loop and monitor the requests\n",
    "    \"\"\"\n",
    "    # Pause the loop\n",
    "    sleep(randint(10,15))\n",
    "    # Throw a warning for non-200 status codes\n",
    "    if response.status_code != 200:\n",
    "        message = warn('Status code: {}'.format(response.status_code))\n",
    "        return message\n",
    "\n",
    "def get_title_date():\n",
    "    \"\"\"\n",
    "    Gets the title and the release date\n",
    "    \"\"\"\n",
    "    # The title\n",
    "    title = url.replace('/tv/', '').replace('/', ': ').replace('-', ' ')\n",
    "    # The release date\n",
    "    release_date = page_html.find('span', class_='release_date').find('span', class_=None).text\n",
    "    return title, release_date\n",
    "\n",
    "def get_user_rating_review():\n",
    "    \"\"\"\n",
    "    Gets the user name, rating, review, and review_date\n",
    "    Adds the scraped data to the lists\n",
    "    \"\"\"\n",
    "    # Add the title to the list\n",
    "    titles.append(title)\n",
    "    # Add the release date to the list\n",
    "    dates.append(release_date)\n",
    "    # Get the user name and add it to the list\n",
    "    user_name = container.a.text\n",
    "    user_names.append(user_name)\n",
    "    # Get the rating and add it to the list\n",
    "    rating = container.find('div', class_='left fl').text.replace('\\n','')\n",
    "    rating = int(rating)/2\n",
    "    ratings.append(rating)\n",
    "    # Get the review and add it to the list\n",
    "    if container.find('div', class_='review_body'):\n",
    "        review = container.find('div', class_='review_body').text.replace('\\n', '')\n",
    "        reviews.append(review)\n",
    "    else:\n",
    "        reviews.append(' ')\n",
    "    # Get the review date and add it to the list\n",
    "    review_date = container.find('span', class_='date').text\n",
    "    review_dates.append(review_date)\n",
    "    return titles, dates, user_names, ratings, reviews, review_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all episode_urls and get title, date, user names, user ratings, and user reviews\n",
    "# This takes the information for the overview of the episodes\n",
    "\n",
    "# Lists to store the scraped data\n",
    "titles = []\n",
    "dates = []\n",
    "user_names = []\n",
    "ratings = []\n",
    "reviews = []\n",
    "review_dates = []\n",
    "\n",
    "# The loop\n",
    "# For each url of the series\n",
    "for url in episode_urls_list:\n",
    "    # Monitor the url scraped\n",
    "    print(url)\n",
    "    clear_output(wait = True)\n",
    "\n",
    "    # Get the html page\n",
    "    response = requests.get('https://www.metacritic.com'+url+'/user-reviews', headers=headers)\n",
    "    # Monitor the loop\n",
    "    monitor_loop()\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Get the reviews containers\n",
    "    review_containers = page_html.find_all('div', class_='review pad_top1')\n",
    "\n",
    "    # If any review_container, get the data\n",
    "    if review_containers:\n",
    "        # The title and release date\n",
    "        title, release_date = get_title_date()\n",
    "        # Get the data for each review\n",
    "        for container in review_containers:\n",
    "            titles, dates, user_names, ratings, reviews, review_dates = get_user_rating_review()\n",
    "\n",
    "        # Go to the next page (if any)\n",
    "        # Runs only if there is a `next` button\n",
    "        if page_html.find('span', class_='flipper next'):\n",
    "            # Get the page url\n",
    "            next_page = page_html.find('span', class_='flipper next').a\n",
    "            # The while loop runs only if next_page!=None\n",
    "            while next_page:\n",
    "                next_page_url = next_page['href']\n",
    "                # Monitor the page scraped\n",
    "                print(next_page_url)\n",
    "                clear_output(wait = True)\n",
    "                # Download the next page\n",
    "                response = requests.get('https://www.metacritic.com'+next_page_url, headers=headers)\n",
    "                # Monitor the loop\n",
    "                monitor_loop()\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "                # The reviews containers\n",
    "                review_containers = page_html.find_all('div', class_='review pad_top1')\n",
    "                # The data for each review\n",
    "                for container in review_containers:\n",
    "                    titles, dates, user_names, ratings, reviews, review_dates = get_user_rating_review()\n",
    "                # Get the page url\n",
    "                # The while loop stops if next_page==None\n",
    "                next_page = page_html.find('span', class_='flipper next').a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save episode ratings into csv, so that we don't need to make future requests\n",
    "episode_ratings = pd.DataFrame({\n",
    "    'title' : titles,\n",
    "    'release_date' : dates,\n",
    "    'user_name' : user_names,\n",
    "    'rating' : ratings,\n",
    "    'review' : reviews,\n",
    "    'review_date' : review_dates\n",
    "})\n",
    "\n",
    "episode_ratings.to_csv('episode_ratings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning before inserting into the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37701, 6), (1891, 6))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read the season_ratings and series_ratings csv files\n",
    "season = pd.read_csv('season_ratings.csv')\n",
    "series = pd.read_csv('series_ratings.csv')\n",
    "season.shape, series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39592, 6)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatente the dataframes\n",
    "frames = [season, series]\n",
    "df = pd.concat(frames)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I scraped the ratings on the scale 1-10\n",
    "# Divide rating by 2\n",
    "df['rating'] /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            object\n",
       "release_date     object\n",
       "user_name        object\n",
       "rating          float64\n",
       "review           object\n",
       "review_date      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apr 17, 2011                                                                                                                                   1671\n",
       "\\n\\n                                                                CW\\n                                                            \\n         1228\n",
       "Oct 31, 2010                                                                                                                                   1216\n",
       "\\n\\n                                                                Netflix\\n                                                            \\n    1058\n",
       "\\n\\n                                                                BBC\\n                                                            \\n         796\n",
       "                                                                                                                                               ... \n",
       "June 5, 2011                                                                                                                                      1\n",
       "April 10, 2016                                                                                                                                    1\n",
       "September 13, 1996                                                                                                                                1\n",
       "November 30, 1988                                                                                                                                 1\n",
       "September 20, 2006                                                                                                                                1\n",
       "Name: release_date, Length: 867, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release date and review_date should be date.\n",
    "# Inspect release_date\n",
    "df['release_date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data and save only the release year when possible\n",
    "years = []\n",
    "for d in df['release_date'].tolist():\n",
    "    try:\n",
    "        year = d.split(', ')[1]\n",
    "    except:\n",
    "        year = None\n",
    "    years.append(year)\n",
    "df['release_date'] = years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011    4024\n",
       "2016    2751\n",
       "2017    2379\n",
       "2013    2232\n",
       "2014    2125\n",
       "2005    2033\n",
       "2015    1884\n",
       "2010    1836\n",
       "2018    1466\n",
       "2009    1256\n",
       "2007    1253\n",
       "2006     859\n",
       "2012     701\n",
       "2008     693\n",
       "1999     575\n",
       "2002     506\n",
       "2020     493\n",
       "2001     427\n",
       "2004     417\n",
       "2019     341\n",
       "1989     267\n",
       "1997     262\n",
       "2000     181\n",
       "1993     166\n",
       "1994     159\n",
       "2003     148\n",
       "1998      64\n",
       "1988      34\n",
       "1990      26\n",
       "1991      24\n",
       "1996      20\n",
       "1975      15\n",
       "1995       5\n",
       "1944       5\n",
       "1982       3\n",
       "1981       1\n",
       "1969       1\n",
       "Name: release_date, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['release_date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apr  1, 2006     3\n",
       "Apr  1, 2007     3\n",
       "Apr  1, 2008     2\n",
       "Apr  1, 2009     3\n",
       "Apr  1, 2010     1\n",
       "                ..\n",
       "Sep 30, 2015     7\n",
       "Sep 30, 2016     9\n",
       "Sep 30, 2017    11\n",
       "Sep 30, 2018    10\n",
       "Sep 30, 2019     6\n",
       "Name: review_date, Length: 5008, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect review_date\n",
    "df['review_date'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataframe\n",
    "df.to_csv('clean_ratings.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>user_name</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Unbreakable Kimmy Schmidt: Kimmy Vs. The Rever...</td>\n",
       "      <td>2020</td>\n",
       "      <td>remyzerofan</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I haven't smiled a lot lately because the quar...</td>\n",
       "      <td>May 12, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Solar Opposites: Season 1</td>\n",
       "      <td>2020</td>\n",
       "      <td>ManvsCar</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Thought it was pretty good, not as good as ric...</td>\n",
       "      <td>May  9, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Solar Opposites: Season 1</td>\n",
       "      <td>2020</td>\n",
       "      <td>Chunklite</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Really funny and crazy, it only gets better as...</td>\n",
       "      <td>May  8, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Solar Opposites: Season 1</td>\n",
       "      <td>2020</td>\n",
       "      <td>kirkender</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Rick and Morty comparisons aside, Solar Opposi...</td>\n",
       "      <td>May  9, 2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Solar Opposites: Season 1</td>\n",
       "      <td>2020</td>\n",
       "      <td>Jakepuwawa</td>\n",
       "      <td>4.0</td>\n",
       "      <td>It's really dumb but at the same time interest...</td>\n",
       "      <td>May  8, 2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title release_date  \\\n",
       "0  Unbreakable Kimmy Schmidt: Kimmy Vs. The Rever...         2020   \n",
       "1                          Solar Opposites: Season 1         2020   \n",
       "2                          Solar Opposites: Season 1         2020   \n",
       "3                          Solar Opposites: Season 1         2020   \n",
       "4                          Solar Opposites: Season 1         2020   \n",
       "\n",
       "     user_name  rating                                             review  \\\n",
       "0  remyzerofan     5.0  I haven't smiled a lot lately because the quar...   \n",
       "1     ManvsCar     4.0  Thought it was pretty good, not as good as ric...   \n",
       "2    Chunklite     5.0  Really funny and crazy, it only gets better as...   \n",
       "3    kirkender     2.5  Rick and Morty comparisons aside, Solar Opposi...   \n",
       "4   Jakepuwawa     4.0  It's really dumb but at the same time interest...   \n",
       "\n",
       "    review_date  \n",
       "0  May 12, 2020  \n",
       "1  May  9, 2020  \n",
       "2  May  8, 2020  \n",
       "3  May  9, 2020  \n",
       "4  May  8, 2020  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39592"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_reviews_lst = []\n",
    "titles = df['title'].tolist()\n",
    "release_dates = df['release_date'].tolist()\n",
    "user_names = df['user_name'].tolist()\n",
    "ratings = df['rating'].tolist()\n",
    "reviews = df['review'].tolist()\n",
    "review_dates = df['review_date'].tolist()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    tv_reviews_lst.append([titles[i], release_dates[i], user_names[i], ratings[i], reviews[i], review_dates[i]])\n",
    "\n",
    "len(tv_reviews_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ················\n"
     ]
    }
   ],
   "source": [
    "connection = psycopg2.connect(\n",
    "    database  = \"postgres\",\n",
    "    user      = \"postgres\",\n",
    "    password  = getpass(), # secure password entry. Enter DB password in the prompt and press Enter.\n",
    "    host      = \"groa.cbayt2opbptw.us-east-1.rds.amazonaws.com\",\n",
    "    port      = '5432'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing step: 0\n",
      "doing step: 1000\n",
      "doing step: 2000\n",
      "doing step: 3000\n",
      "doing step: 4000\n",
      "doing step: 5000\n",
      "doing step: 6000\n",
      "doing step: 7000\n",
      "doing step: 8000\n",
      "doing step: 9000\n",
      "doing step: 10000\n",
      "doing step: 11000\n",
      "doing step: 12000\n",
      "doing step: 13000\n",
      "doing step: 14000\n",
      "doing step: 15000\n",
      "doing step: 16000\n",
      "doing step: 17000\n",
      "doing step: 18000\n",
      "doing step: 19000\n",
      "doing step: 20000\n",
      "doing step: 21000\n",
      "doing step: 22000\n",
      "doing step: 23000\n",
      "doing step: 24000\n",
      "doing step: 25000\n",
      "doing step: 26000\n",
      "doing step: 27000\n",
      "doing step: 28000\n",
      "doing step: 29000\n",
      "doing step: 30000\n",
      "doing step: 31000\n",
      "doing step: 32000\n",
      "doing step: 33000\n",
      "doing step: 34000\n",
      "doing step: 35000\n",
      "doing step: 36000\n",
      "doing step: 37000\n",
      "doing step: 38000\n",
      "doing step: 39000\n"
     ]
    }
   ],
   "source": [
    "cursor = connection.cursor()\n",
    "step = 1000\n",
    "\n",
    "for ix in range(0, len(df), step):\n",
    "    print(f\"doing step: {ix}\")\n",
    "    \n",
    "    batch = tv_reviews_lst[ix:ix+step]\n",
    "    \n",
    "    execute_batch(cursor, \"\"\"\n",
    "        INSERT INTO tvshow_reviews (title, release_date, user_name, rating, review_text, review_date)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s);\n",
    "        \"\"\", batch\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TVShow Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all the tv urls\n",
    "tv_urls = pd.read_csv('tv_urls.csv')\n",
    "tv_urls_list = tv_urls['tv_url'].tolist()\n",
    "\n",
    "# Lists to store the scraped data in\n",
    "names = names[]\n",
    "dates = dates[]\n",
    "genres = genres[]\n",
    "descriptions = descriptions[]\n",
    "poster_urls = poster_urls[]\n",
    "\n",
    "# For each url\n",
    "for url in tv_urls_list:\n",
    "    # Get the page\n",
    "    response = requests.get('https://www.metacritic.com'+url, headers=headers)\n",
    "\n",
    "    # Pause the loop\n",
    "    sleep(randint(8,15))\n",
    "    # Monitor the loop\n",
    "    print(url)\n",
    "    clear_output(wait = True)\n",
    "\n",
    "    # Parse the content of the request with BeautifulSoup\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # The titles\n",
    "    title = page_html.h1.text\n",
    "    names.append(title)\n",
    "\n",
    "    # The release date\n",
    "    date_container = page_html.find('span', class_='release_date')\n",
    "    release_date = date_container.find('span', class_=None).text\n",
    "    dates.append(release_date)\n",
    "\n",
    "    # The genres\n",
    "    genres_section = page_html.find('div', class_='genres')\n",
    "    if genres_section:\n",
    "        spans = genres_section.find_all('span', class_=None)\n",
    "        genres_list = []\n",
    "        for i in range(1, len(spans)):\n",
    "            genre = spans[i].text.replace('\\n', '')\n",
    "            genres_list.append(genre)\n",
    "        genres.append(genres_list)\n",
    "    else:\n",
    "        genres.append(' ')\n",
    "\n",
    "    # The description\n",
    "    summary_section = page_html.find('div', class_='summary_deck details_section')\n",
    "    description = summary_section.find('span', class_=None).text.replace('\\n', '')\n",
    "    descriptions.append(description)\n",
    "\n",
    "    # The poster url\n",
    "    poster_url = page_html.find('img', class_='summary_img')['src']\n",
    "    poster_urls.append(poster_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into dataframe\n",
    "tvshows = pd.DataFrame({\n",
    "    'url' : urls,\n",
    "    'title': names,\n",
    "    'release_date' : dates,\n",
    "    'genres' : genres,\n",
    "    'description' : descriptions,\n",
    "    'poster_url' : poster_urls\n",
    "})\n",
    "\n",
    "# Save into csv if we need for future processing\n",
    "tvshows.to_csv('tvshows.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert into the database\n",
    "\n",
    "# Prepare the list to be inserted\n",
    "tv_shows_lst = []\n",
    "for i in range(len(tvshows)):\n",
    "    tv_shows_lst.append([names[i], dates[i], descriptions[i], poster_urls[i], genres[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect\n",
    "connection = psycopg2.connect(\n",
    "    database  = \"postgres\",\n",
    "    user      = \"postgres\",\n",
    "    password  = getpass(), # secure password entry. Enter DB password in the prompt and press Enter.\n",
    "    host      = \"groa.cbayt2opbptw.us-east-1.rds.amazonaws.com\",\n",
    "    port      = '5432'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert\n",
    "cursor = connection.cursor()\n",
    "step = 100\n",
    "\n",
    "for ix in range(0, len(tv_shows_lst), step):\n",
    "    print(f\"doing step: {ix}\")\n",
    "    \n",
    "    batch = tv_shows_lst[ix:ix+step]\n",
    "    \n",
    "    execute_batch(cursor, \"\"\"\n",
    "        INSERT INTO tvshows (title, release_date, description, poster_url, genres)\n",
    "        VALUES (%s, %s, %s, %s, %s);\n",
    "        \"\"\", batch\n",
    "    )\n",
    "cursor.close()\n",
    "connection.commit()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Metacritic-scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
